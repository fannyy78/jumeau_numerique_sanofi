{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from branca.element import MacroElement\n",
    "from jinja2 import Template\n",
    "from IPython.display import IFrame\n",
    "import webbrowser\n",
    "\n",
    "from libpysal.weights import Queen\n",
    "from esda.moran import Moran\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Charger les données sauvegardées\n",
    "gdf_factors = gpd.read_file(\"gdf_factors.geojson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_factors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : Nettoyage des géométries\n",
    "gdf_factors = gdf_factors[gdf_factors.geometry.notnull()]\n",
    "gdf_factors = gdf_factors[gdf_factors.is_valid]\n",
    "\n",
    "# Étape 2 : Supprimer les géométries de type Point ou LineString si présentes\n",
    "gdf_factors = gdf_factors[gdf_factors.geometry.geom_type.isin(['Polygon', 'MultiPolygon'])]\n",
    "\n",
    "# Étape 3 : Sélection des colonnes numériques avec variance non nulle\n",
    "numeric_columns = gdf_factors.select_dtypes(include='number').columns\n",
    "numeric_columns = [col for col in numeric_columns if gdf_factors[col].std() > 0]\n",
    "\n",
    "# Étape 4 : Création de la matrice de voisinage spatiale\n",
    "w = Queen.from_dataframe(gdf_factors, use_index=True)\n",
    "w.transform = 'r'\n",
    "\n",
    "# Étape 5 : Calcul de Moran's I pour chaque facteur\n",
    "results = []\n",
    "for col in numeric_columns:\n",
    "    try:\n",
    "        moran = Moran(gdf_factors[col], w)\n",
    "        results.append({\n",
    "            'facteur': col,\n",
    "            \"Moran's I\": round(moran.I, 4),\n",
    "            'p-value': round(moran.p_sim, 4)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            'facteur': col,\n",
    "            \"Moran's I\": None,\n",
    "            'p-value': None,\n",
    "            'erreur': str(e)\n",
    "        })\n",
    "\n",
    "# Résultats\n",
    "moran_df = pd.DataFrame(results)\n",
    "print(moran_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagramme des indices de Moran\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=moran_df.sort_values(\"Moran's I\", ascending=False), x=\"Moran's I\", y=\"facteur\", palette=\"viridis\")\n",
    "plt.title(\"Indice de Moran par facteur\")\n",
    "plt.xlabel(\"Moran's I\")\n",
    "plt.ylabel(\"Facteur\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"moran_barplot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "facteurs = [\n",
    "    'demographie_niveau_vie', 'emploi_activite', 'acces_sante',\n",
    "    'services_publics', 'agriculture_elevage', 'industrie_agro_chimie',\n",
    "    'environnement_energie', 'risques_industriels'\n",
    "]\n",
    "\n",
    "print(\"Colonnes dans gdf_factors :\", gdf_factors.columns.tolist())\n",
    "\n",
    "facteurs_valides = [f for f in facteurs if f in gdf_factors.columns]\n",
    "print(\"facteurs_valides :\", facteurs_valides)\n",
    "\n",
    "results = []\n",
    "\n",
    "for target in facteurs_valides:\n",
    "    features = [f for f in facteurs_valides if f != target]\n",
    "    print(f\"\\nModèle pour cible : {target} avec features : {features}\")\n",
    "\n",
    "    X = gdf_factors[features]\n",
    "    y = gdf_factors[target]\n",
    "\n",
    "    data = pd.concat([X, y], axis=1).dropna()\n",
    "    print(f\"Données valides (sans NaN) pour {target} :\", data.shape)\n",
    "\n",
    "    if data.empty:\n",
    "        print(f\"Aucune donnée valide pour {target}, on passe à la cible suivante.\")\n",
    "        continue\n",
    "\n",
    "    X_clean = data[features]\n",
    "    y_clean = data[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, random_state=42)\n",
    "\n",
    "    # Entraînement des modèles\n",
    "    lr = LinearRegression().fit(X_train, y_train)\n",
    "    rf = RandomForestRegressor(random_state=42).fit(X_train, y_train)\n",
    "    hgb = HistGradientBoostingRegressor(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "    # Prédictions et scores R²\n",
    "    r2_lr = r2_score(y_test, lr.predict(X_test))\n",
    "    r2_rf = r2_score(y_test, rf.predict(X_test))\n",
    "    r2_hgb = r2_score(y_test, hgb.predict(X_test))\n",
    "\n",
    "    # Déterminer le meilleur modèle\n",
    "    scores = {'LinReg': r2_lr, 'RandomForest': r2_rf, 'HistGradientBoosting': r2_hgb}\n",
    "    best_model_name = max(scores, key=scores.get)\n",
    "    best_r2 = scores[best_model_name]\n",
    "\n",
    "    print(f\"Meilleur modèle pour {target} : {best_model_name} avec R² = {best_r2:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'Cible': target,\n",
    "        'R2_LinReg': r2_lr,\n",
    "        'R2_RandomForest': r2_rf,\n",
    "        'R2_HistGradientBoosting': r2_hgb,\n",
    "        'Best_Model': best_model_name,\n",
    "        'Best_R2': best_r2\n",
    "    })\n",
    "\n",
    "    # Affichage de l'importance des variables selon le meilleur modèle\n",
    "    if best_model_name == 'LinReg':\n",
    "        # Importance via valeur absolue des coefficients\n",
    "        coefs = pd.Series(abs(lr.coef_), index=X_train.columns).sort_values()\n",
    "        coefs.plot(kind='barh', title=f\"Importance des facteurs pour {target} ({best_model_name})\")\n",
    "        plt.xlabel(\"Valeur absolue des coefficients\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    elif best_model_name == 'RandomForest':\n",
    "        # Permutation importance pour RF\n",
    "        result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "        importances = pd.Series(result.importances_mean, index=X_test.columns).sort_values()\n",
    "        importances.plot(kind='barh', title=f\"Importance des facteurs pour {target} ({best_model_name} - permutation)\")\n",
    "        plt.xlabel(\"Importance moyenne par permutation\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:  # HistGradientBoosting\n",
    "        result = permutation_importance(hgb, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "        importances = pd.Series(result.importances_mean, index=X_test.columns).sort_values()\n",
    "        importances.plot(kind='barh', title=f\"Importance des facteurs pour {target} ({best_model_name} - permutation)\")\n",
    "        plt.xlabel(\"Importance moyenne par permutation\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nRésumé des scores R² et meilleurs modèles :\")\n",
    "    print(results_df)\n",
    "else:\n",
    "    print(\"Aucun résultat à afficher (pas de données valides ou pas de facteurs valides).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# XGBoost et CatBoost si installés\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgb_installed = True\n",
    "except ImportError:\n",
    "    xgb_installed = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost_installed = True\n",
    "except ImportError:\n",
    "    catboost_installed = False\n",
    "\n",
    "facteurs = [\n",
    "    'demographie_niveau_vie', 'emploi_activite', 'acces_sante',\n",
    "    'services_publics', 'agriculture_elevage', 'industrie_agro_chimie',\n",
    "    'environnement_energie', 'risques_industriels'\n",
    "]\n",
    "\n",
    "facteurs_valides = [f for f in facteurs if f in gdf_factors.columns]\n",
    "results = []\n",
    "\n",
    "for target in facteurs_valides:\n",
    "    features = [f for f in facteurs_valides if f != target]\n",
    "    print(f\"\\nCible : {target}\")\n",
    "\n",
    "    X = gdf_factors[features].copy()\n",
    "    y = gdf_factors[target]\n",
    "\n",
    "    # 1️⃣ Imputation KNN\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # 2️⃣ Création d'interactions simples et triples\n",
    "    cols = X_imputed.columns.tolist()\n",
    "    # Interactions simples\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            X_imputed[f'{cols[i]}*{cols[j]}'] = X_imputed[cols[i]] * X_imputed[cols[j]]\n",
    "    # Interactions triples (si dataset pas trop petit)\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            for k in range(j+1, len(cols)):\n",
    "                X_imputed[f'{cols[i]}*{cols[j]}*{cols[k]}'] = X_imputed[cols[i]] * X_imputed[cols[j]] * X_imputed[cols[k]]\n",
    "\n",
    "    # 3️⃣ Transformations log1p et sqrt\n",
    "    for col in X_imputed.columns:\n",
    "        if (X_imputed[col] > 0).all():\n",
    "            X_imputed[f'log1p_{col}'] = np.log1p(X_imputed[col])\n",
    "            X_imputed[f'sqrt_{col}'] = np.sqrt(X_imputed[col])\n",
    "\n",
    "    # 4️⃣ Standardisation\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "    # 5️⃣ Sélection automatique des 10 meilleures features\n",
    "    k_best = min(10, X_scaled.shape[1])\n",
    "    selector = SelectKBest(score_func=f_regression, k=k_best)\n",
    "    X_selected = selector.fit_transform(X_scaled, y)\n",
    "    selected_features = [X_imputed.columns[i] for i, s in enumerate(selector.get_support()) if s]\n",
    "\n",
    "    # 6️⃣ Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_selected, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # 7️⃣ Définition des modèles de base pour stacking\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(random_state=42, n_estimators=200, max_depth=5)),\n",
    "        ('hgb', HistGradientBoostingRegressor(random_state=42, max_iter=200))\n",
    "    ]\n",
    "    if xgb_installed:\n",
    "        estimators.append(('xgb', XGBRegressor(random_state=42, n_estimators=200, max_depth=5, learning_rate=0.1)))\n",
    "    if catboost_installed:\n",
    "        estimators.append(('cat', CatBoostRegressor(random_state=42, iterations=200, verbose=0)))\n",
    "\n",
    "    # 8️⃣ Stacking\n",
    "    stack_model = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LinearRegression(),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    stack_model.fit(X_train, y_train)\n",
    "\n",
    "    # 9️⃣ Évaluation\n",
    "    y_pred = stack_model.predict(X_test)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    cv_r2 = cross_val_score(stack_model, X_selected, y, cv=5, scoring='r2').mean()\n",
    "    print(f\"Stacking R² test = {test_r2:.4f}, CV R² = {cv_r2:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'Cible': target,\n",
    "        'R2_Test': test_r2,\n",
    "        'R2_CV': cv_r2,\n",
    "        'Best_Model': 'Stacking'\n",
    "    })\n",
    "\n",
    "    # 10️⃣ Importance des variables\n",
    "    perm = permutation_importance(stack_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    importances = pd.Series(perm.importances_mean, index=selected_features).sort_values()\n",
    "    importances.plot(kind='barh', title=f\"Importance pour {target} (Stacking Ultra)\")\n",
    "    plt.xlabel(\"Importance moyenne par permutation\")\n",
    "    plt.show()\n",
    "\n",
    "# Résumé final\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nRésumé des R² et meilleurs modèles :\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
